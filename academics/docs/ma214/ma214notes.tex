\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	

%common math and LaTeX packages
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.0in}
\usepackage{empheq}

\setcounter{tocdepth}{1}


%shaded environment for important equations/notes
\usepackage{mdframed}
\colorlet{shaded}{blue!15}
\colorlet{shadedtext}{black}
\newenvironment{shaded}
   {
     \raggedright
     \color{shadedtext}%
   }{}
\surroundwithmdframed[
   hidealllines=true,
   backgroundcolor=shaded
]{shaded}

%page geometry definitions
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\parindent 0in
\parskip 6pt
\geometry{margin=1in, headsep=0.25in}

%custom theorem definitions
\theoremstyle{definition}
\newtheorem{innercustomgeneric}{\customgenericname}
\providecommand{\customgenericname}{}
\newcommand{\newcustomtheorem}[2]{%
  \newenvironment{#1}[1]
  {%
   \renewcommand\customgenericname{#2}%
   \renewcommand\theinnercustomgeneric{##1}%
   \innercustomgeneric
  }
  {\endinnercustomgeneric}
}
\newcustomtheorem{thm}{Theorem}
\newcustomtheorem{lem}{Lemma}
\newcustomtheorem{defn}{Definition}
\newcustomtheorem{prop}{Proposition}
\newcustomtheorem{exer}{Exercise}
\newcustomtheorem{note}{Note}
\renewcommand{\qedsymbol}{$\blacksquare$}

\let\a\alpha
\let\b\beta
\let\g\gamma
\let\e\varepsilon
\let\t\theta
\let\phi\varphi
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Lagr}{\mathcal{L}}

\begin{document}

%document header
\begin{center}
{\LARGE \bf MA 214 - Introduction to Numerical Analysis}\\
{Instructor: \textit{Prof. Saikat Mazumdar}}\\
Last updated \today \\~\\
{\large \bf Om Prabhu}\\
Undergraduate, Mechanical Engineering\\
Indian Institute of Technology Bombay\\~\\
\textsc{Note To Reader}
\end{center}
\vspace{-6pt}

This document is a compilation of the notes I made while taking the course MA 214 (Introduction to Numerical Analysis) in my 4$^{\text{th}}$ semester at IIT Bombay. It is not meant to serve as a replacement for any formal textbook or lecture on the subject, since I sometimes overlook the theory parts.

There will probably be many instances where I use certain symbols without explicitly mentioning what they mean. It is to be assumed that they carry their usual meanings. I may also change the order of notes compared to those in the slides if I find it more convenient.

If you have any suggestions and/or spot any errors, you know where to contact me.

\hrulefill
\vspace{-5mm}
\tableofcontents
\hrulefill
\vspace{2mm}

{\large \bf Notation}
\vspace{2mm}

\begin{tabular}{p{0.1\textwidth}p{0.85\textwidth}}
$\mathbb{P}_n$ & set of all polynomials of degree $\leqslant n$\\
$\mathcal{C}[a,b]$ & set of all continuous functions on $[a,b]$ (an infinite dimensional vector space)\\
$\mathcal{C}^n[a,b]$ & set of all continuous functions with continuous $n^{th}$ order derivative on $[a,b]$ 
\end{tabular}
\vspace{-2mm}
 
\hrulefill

\newpage
\section{Interpolation Theory}
Suppose $(n+1)$ real points $(x_0,y_0), (x_1,y_1),\dots,(x_n,y_n)$ are known. Further these \textbf{interpolation points} $x_i$ are spread out over the interval $[a,b]$. Then the problem of approximating a function over the interval $[a,b]$ passing through these points is called interpolation.

There are infinite such functions. We mainly consider polynomial interpolation in this section i.e. we approximate the \textbf{interpolant} $f$ by an \textbf{interpolating polynomial} $p_n\in\mathbb{P}_n$.

\subsection{Some existence theorems}

\begin{enumerate}
	\item The \textbf{Joseph-Louis Lagrange Theorem} states that given a set of $n+1$ real, unique data points $S=\{(x_i,y_i)\text{ }|\text{ }i=0,1,\dots,n\}$, there exists a unique polynomial $p_n\in\mathbb{P}_n$ such that $$p(x_i)=y_i\text{ for }i=0,1,\dots,n$$
\end{enumerate}
We define the norm on $\mathcal{C}[a.b]$ as:
$\displaystyle||f||=\max_{x\in[a,b]}|f(x)|$. To define the `closeness' of 2 functions formally, we consider the quantity
$\displaystyle||f-g||=\max_{x\in [a,b]}\left|f(x)-g(x)\right|$. 
\begin{enumerate}
	\setcounter{enumi}{1}
	\item  Take a function $f\in\C[a,b]$. The \textbf{Weierstrass Approximation Theorem} states that given any real number $\e>0$, there exists a polynomial $p$ such that 
$$||f-p||<\e\implies |f(x)-p(x)|<\e\text{\hspace{12pt}}\forall x\in[a,b]$$
\end{enumerate}

\subsection{Lagrange interpolation formula} 
Given $n+1$ distinct real points $x_0,x_1,\dots,x_n$ and $n+1$ real numbers $y_0,y_1,\dots,y_n$, there exists a unique polynomial $p_n\in\mathbb{P}_n$ such that $p(x_i)=y_i$ for $ i=0,1,\dots,n$. Construct $n^{th}$ degree polynomials $L_0^n(x),L_1^n(x),\dots,L_n^n(x)$ such that
$$L_k^n(x_i)=\left\lbrace\begin{array}{lr}
1 & \text{if } i=k\\
0 & \text{if } i\neq k
\end{array}\right.\implies \boxed{p_n(x):=\sum_{k=0}^ny_kL_k^n(x)}$$
The lagrange polynomials $L_k^n$ can be found using the following algorithm
$$\boxed{L_k^n(x):=\prod_{j=0,j\neq k}^n\frac{(x-x_j)}{(x_k-x_j)}}$$
\textsc{Note:} As will be seen later, the method of divided differences can also be used for polynomial interpolation. A little bit of manipulation on the Lagrange interpolation formula gives us an alternative way to calculate the divided difference $f[x_0,x_1,\dots,x_n]$, given by $$f[x_0,x_1,\dots,x_n]:=\sum_{k=0}^nf(x_k)\prod_{j=0,j\neq k}^{n}\frac{1}{x-x_j}$$
\newpage
\subsection{Newton's divided differences}
Let $x_0,x_1,\dots,x_n$ be $n+1$ real distinct points in $[a,b].$ Let $f:[a,b]\to\R$ be a function whose values are known at these points. We want to find a polynomial $p_n(x)\in\mathbb{P}_n$ such that $p_n(x_i)=f(x_i)$ for $i=0,1,\dots,n$.

We define the divided differences (independant of order of points) using the recursive relation: $$f[x_0]:=f(x_0)$$$$
f[x_0,x_1,\dots,x_{m+1}]:=\frac{f[x_1,\dots,x_{m+1}]-f[x_0,\dots,x_m]}{x_{m+1}-x_0}$$
Then the polynomial $p_n(x)$ can be written as:
$$\boxed{p_n(x) := f[x_0]  + f[x_0, x_1](x - x_0)+\cdots+f[x_0,x_1,\dots,x_n]\prod_{k=0}^{n-1}(x-x_k)}$$

\subsection{Matrix representation}
The problem of interpolation can also be expressed as a system of linear equations and solved for the coefficients. A matrix similar to the Vandermonde matrix is generated.

$$\begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^n\\
1 & x_1 & x_1^2 & \cdots & x_1^n\\
\vdots & & \ddots & & \vdots\\
1 & x_n & x_n^2 & \cdots & x_n^n\\
\end{bmatrix}
\begin{bmatrix}
a_0\\
a_1\\
\vdots\\
a_n
\end{bmatrix}=
\begin{bmatrix}
y_0\\
y_1\\
\vdots\\
y_n
\end{bmatrix}
$$
\subsection{Error estimation}
\label{subsec:error_eq}
Take $f\in\C^{n+1}[a,b]$. Let $x_0,x_1,\dots,x_n$ be $n+1$ distinct points in $[a,b]$. Let $p\in\mathbb{P}_n$ such that $p(x_i)=f(x_i)$ for $i=1,2,\dots,n$. Then for all $x\in[a,b]$, there exists $\xi=\xi(x)\in (a,b)$ such that
$$f(x)-p(x)=\frac{1}{(n+1)!}f^{(n+1)}(\xi)\prod_{k=0}^n (x-x_k)$$
Taking maximum over $x\in[a,b]$, we can see that our choice of interpolation points influences the error significantly.
$$\max_{x\in[a,b]}|f(x)-p(x)|\leqslant\frac{1}{(n+1)!}||f^{(n+1)}||\max_{x\in[a,b]}\prod_{k=0}^n|(x-x_k)|$$
This invokes the concept of \textbf{Chebyshev's interpolation points}. These are essentially the vertical projections of equally spaced points on a half-circle with center $\displaystyle\frac{a+b}{2}$ and radius $\displaystyle\frac{b-a}{2}$, given by $$x_k=\frac{a+b}{2}+\frac{b-a}{2}\cos\left(\frac{k\pi}{n}\right)$$
\newpage
\subsection{Piecewise interpolation}
A function $\phi\in\C[a,b]$ is a \textbf{piecewise polynomial} on $[a,b]$ if
\vspace{-3mm}
\begin{itemize}
\itemsep0em
	\item[$-$] there exist points $\{x_i\}_{i=0}^n$ such that $a=x_0<x_1<\dots<x_n=b$
	\item[$-$] $\phi\in\PP_m$ is defined in each interval $[x_{i-1},x_i]$ but not necessarily on the entire domain 
	\item[$-$] $m\leqslant n$ and $m\geqslant 0$
\end{itemize}
\vspace{-2mm}
Piecewise interpolation involves building a function $\phi\in\C[a,b]$ such that $\phi\in\PP_n$ on $[x_{i-1},x_i]$ and $\phi(x_{i-1})=f_{i-1}\text{ and }\psi(x_i)=f_i$. The general algorithm for piecewise interpolation is:
\vspace{-3mm}
\begin{itemize}
\itemsep0em
	\item[$-$] pick data points $\{(x_i,f_i)\text{ }|\text{ }i=0,1,\dots,n\}$ such that $a=x_0<x_1<\dots<x_n=b$
	\item[$-$] build $\phi\in\C[a,b]$ on each $[x_{i-1},x_i]$ such that $\phi\in\PP_m[x_{i-1},x_i]$ and $\phi(x_{i-1})=f_{i-1}$
	$$\phi(x_i)=f(x_i)=f_i\text{ for }i=0,1,\dots,n\rightarrow(n+1)\text{ conditions}$$
	$$\phi(x)=\left\lbrace\begin{array}{rl}
		a_0^{(1)}+a_1^{(1)}x+\cdots+a_m^{(1)}x^m & \text{ on }[x_0,x_1]\\
		a_0^{(2)}+a_1^{(2)}x+\cdots+a_m^{(2)}x^m & \text{ on }[x_1,x_2]\\
		\vdots & \\
		a_0^{(n)}+a_1^{(n)}x+\cdots+a_m^{(n)}x^m & \text{ on }[x_{n-1},x_n]
	\end{array}\right\rbrace\text{ }n(m+1)\text{ coefficients}$$
	\item[$-$] continuity of derivatives on interior points $\{x_i\mid i=1,2,\dots,n-1\}$
	$$\left.\begin{array}{c}
	\displaystyle\lim_{h\to 0^{+}}\phi(x_i-h)=\lim_{h\to 0^{+}}\phi(x_i+h)\\
	\displaystyle\lim_{h\to 0^{+}}\phi^1(x_i-h)=\lim_{h\to 0^{+}}\phi^1(x_i+h)\\
	\vdots\\
	\displaystyle\lim_{h\to 0^{+}}\phi^{m-1}(x_i-h)=\lim_{h\to 0^{+}}\phi^{m-1}(x_i+h)\\
	\end{array}\right\rbrace\text{ }m(n-1)\text{ more conditions}$$
	\item[$-$] still need $(m-1)$ more conditions
\end{itemize}
\subsection{Linear interpolating splines}
Take $n+1$ points such that $a=x_0<x_1<\dots<x_n=b$ and a function $f\in\C[a,b]$. The linear interpolating spline $s_L(x)$ is $$\boxed{s_L(x)=\left(\frac{x_i-x}{x_i-x_{i-1}}\right)f_{i-1}+\left(\frac{x-x_{i-1}}{x_i-x_{i-1}}\right)f_i}$$
This is nothing different from connecting each pair of consecutive points with a straight line. Clearly, there will be some error in interpolation since we are approximating $f$ by a set of polynomials in $\PP_1$. The error bound can be quantified as
$$||f-s_L||\leqslant \frac{h^2}{8}||f^{\prime\prime}||\text{ }\text{ }\text{ }\text{ }\text{ where }h=\max_{1\leqslant i\leqslant n}h_i=\max_{1\leqslant i\leqslant n}(x_i-x_{i-1})$$
The proof relies on the error equation introduced in \hyperref[subsec:error_eq]{Section 1.5}. Substitute $n=1$ and note how $\max|(x-x_{i-1})(x-x_i)|=h_i^2/4$ where $h_i=x_i-x_{i-1}$. Finally take a maximum over all the $i'$s.
\newpage
\subsection{Cubic splines}
This is another case of spline interpolation where $s\in\C^2[x_0,x_n]$ such that $s\in\PP_3$ on each $[x_{i},x_{i+1}]$.
\vspace{-8mm}
\begin{itemize}
	\itemsep0em
	\item[$-$] interpolation conditions:
	\vspace{-3.5mm}
	\begin{align*}
		\text{function value}\rightarrow &\left\lbrace\begin{array}{l}
		s_i(x_i)=f_i\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\text{ }\hspace{1mm}\text{ for }i=0,1,\dots,n-1\\
		s_{n-1}(x_n)=f_n
	\end{array}\right.\\
	\text{continuity of }s\rightarrow & \text{ }\text{ }\text{ }\text{ }s_i(x_{i+1})=s_{i+1}(x_{i+1})\text{ }\text{ }\text{ }\text{ for }i=0,1,\dots,n-2\\
	\text{continuity of }s^{\prime}\rightarrow & \text{ }\text{ }\text{ }\text{ }s_i^{\prime}(x_{i+1})=s_{i+1}^{\prime}(x_{i+1})\text{ }\text{ }\text{ }\text{ for }i=0,1,\dots,n-2\\
	\text{continuity of }s^{\prime\prime}\rightarrow &  \text{ }\text{ }\text{ }\text{ }s_i^{\prime\prime}(x_{i+1})=s_{i+1}^{\prime\prime}(x_{i+1})\text{ }\hspace{2mm}\text{ for }i=0,1,\dots,n-2
	\end{align*}
	\vspace{-7mm}
	\item[$-$] take polynomials of the form $s_i(x)=a_i(x-x_i)^3+b_i(x-x_i)^2+c_i(x-x_i)+d_i$ for $x\in[x_{i},x_{i+1}]$ and $i=0,1,\dots,n-1$
	\item[$-$] $4n$ coefficients \& $4n-2$ conditions, need 2 more conditions $\rightarrow s_0^{\prime\prime}(x_0)=s_{n-1}^{\prime\prime}(x_n)=0$
\end{itemize}
\vspace{-2mm}

Instead of solving a $4n\times 4n$ matrix, we can make our life a little easier. Take equally spaced knots $h=|x_{i+1}-x_i|$ for $i=0,1,\dots, n-1$. Using the general form for $s_i(x)$, we get $$s_i(x_i)=f_i\implies \boxed{d_i=f_i}\text{ }\text{ }\text{ }\text{ for }i=0,1,\dots,n-1$$
We further define new variables as $\sigma_i=s^{\prime\prime}(x_i)$ for $i=0,1,\dots,n$. We already know $\sigma_0=\sigma_n=0$, thus we have $n-1$ unknown quantities. We have
\begin{equation}
s_i^{\prime\prime}(x)=6a_i(x-x_i)+2b_i\implies \sigma_i=s_i^{\prime\prime}(x_i)=2b_i\implies \boxed{b_i=\frac{\sigma_i}{2}}
\end{equation}
Using the condition that $s_i^{\prime\prime}(x_{i+1})=s_{i+1}^{\prime\prime}(x_{i+1})$, we have
\begin{equation}
6a_i(x_{i+1}-x_i)+2b_i=\sigma_{i+1}\implies \boxed{a_i=\frac{\sigma_{i+1}-\sigma_i}{6h}}
\end{equation}
Next, we evaluate $s_i(x)$ at $x=x_{i+1}$ to get
\begin{equation}
f_{i+1}=s_i(x_{i+1})=a_ih^3+b_ih^2+c_ih+d_i\implies \boxed{c_i=\frac{f_{i+1}-f_i}{h}-\frac{h}{6}(2\sigma_i+\sigma_{i+1})}
\end{equation}
Finally using the continuity of $s^{\prime}$ i.e. $s_i^{\prime}(x_{i+1})=s_{i+1}^{\prime}(x_{i+1})$, we get
$$\left.\begin{array}{r}
	s_i^{\prime}(x)=3a_i(x-x_i)^2+2b_i(x-x_i)+c_i\\
	s_{i+1}^{\prime}(x)=3a_{i+1}(x-x_i)^2+2b_{i+1}(x-x_i)+c_{i+1}
\end{array}\right\rbrace\implies 3a_ih^2+2b_ih+c_i=c_{i+1}$$
A little bit of careful manipulation using equations (1), (2) and (3) yields us the recursive relation for $i=1,\dots,n-1$: $$\sigma_{i-1}+4\sigma_i+\sigma_{i+1}=\dfrac{6}{h^2}(f_{i-1}-2f_i+f_{i+1})\left\lbrace\arraycolsep=2pt\def\arraystretch{1.8}
\begin{array}{l}
	\sigma_{0}+4\sigma_1+\sigma_{2}=\dfrac{6}{h^2}(f_{0}-2f_1+f_{2})\\
	\sigma_{1}+4\sigma_2+\sigma_{3}=\dfrac{6}{h^2}(f_{1}-2f_2+f_{3})\\
	\vdots\\
	\sigma_{n-2}+4\sigma_{n-1}+\sigma_{n}=\dfrac{6}{h^2}(f_{n-2}-2f_{n-1}+f_{n})\\
\end{array}\right.
$$
This system of equations can be expressed as a matrix equation which is more convenient to solve:
$$\begin{bmatrix}
	4 & 1 & 0 & \cdots & 0 & 0 & 0\\ 
	1 & 4 & 1 & \cdots & 0 & 0 & 0\\ 
	0 & 1 & 4 & \cdots & 0 & 0 & 0\\ 
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\ 
	0 & 0 & 0 & \cdots & 4 & 1 & 0\\ 
	0 & 0 & 0 & \cdots & 1 & 4 & 1\\ 
	0 & 0 & 0 & \cdots & 0 & 1 & 4\\ 
\end{bmatrix}
\begin{bmatrix}
	\sigma_1\\
	\sigma_2\\
	\sigma_3\\
	\vdots\\
	\sigma_{n-3}\\
	\sigma_{n-2}\\
	\sigma_{n-1}\\
\end{bmatrix}=\frac{6}{h^2}\begin{bmatrix}
	f_0 - 2f_1+f_2\\
	f_1 - 2f_2+f_3\\
	f_2 - 2f_3+f_4\\	
	\vdots\\
	f_{n-4}-2f_{n-3}+f_{n-2}\\
	f_{n-3}-2f_{n-2}+f_{n-1}\\
	f_{n-2}-2f_{n-1}+f_{n}\\ 
\end{bmatrix}
$$
As with linear splines, there is also an error bound associated with cubic splines. This is given by
$$||f-s||\leqslant Ch^4||f^{(iv)}||\text{ }\text{ }\text{ where }h=\max_{1\leqslant i\leqslant n}h_i=\max_{1\leqslant i\leqslant n}(x_i-x_{i-1})\text{ and } C=\text{constant}$$
\end{document}